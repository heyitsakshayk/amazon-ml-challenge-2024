{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb8c90d-44c3-4ec6-8bcc-633843c7fcb5",
   "metadata": {},
   "source": [
    "# Submission 1 of Team AIvengers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dd29a0-63ce-4f56-a234-b78250a840ff",
   "metadata": {},
   "source": [
    "By Akshay Kumar, Anubroto Ghose and Devina Goel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062dc8f-a6d3-4000-8899-df3b2c7b9735",
   "metadata": {},
   "source": [
    "# Testing if CUDA is configured properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89807eba-7a0e-40e3-9703-d33aa4de249f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e49d8-4d83-4056-a47d-a71d1dcbbb09",
   "metadata": {},
   "source": [
    "# This is the main function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036c964-cf5e-42bd-bb12-1374fdfb0418",
   "metadata": {},
   "source": [
    "In this iteration of our solution, the Width Height and Depth predictions have been excluded. We will attempt that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1955aa0d-48ca-469e-87dd-5d3c383cc0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay Kumar\\anaconda3\\Lib\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "C:\\Users\\Akshay Kumar\\anaconda3\\Lib\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import easyocr \n",
    "import argparse\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "reader = easyocr.Reader(['en'],gpu=True)\n",
    "\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram', 'kilogram', 'microgram', 'milligram', 'ounce', 'pound', 'ton'},\n",
    "    'maximum_weight_recommendation': {'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre',\n",
    "        'cubic foot',\n",
    "        'cubic inch',\n",
    "        'cup',\n",
    "        'decilitre',\n",
    "        'fluid ounce',\n",
    "        'gallon',\n",
    "        'imperial gallon',\n",
    "        'litre',\n",
    "        'microlitre',\n",
    "        'millilitre',\n",
    "        'pint',\n",
    "        'quart'}\n",
    "}\n",
    "\n",
    "# Separate functions for each key in the entity_unit_map\n",
    "\n",
    "def handle_width(image, category_id):\n",
    "    unit_map = {\n",
    "        'centimetre': ['cm', 'centimetre', 'centimeter'],\n",
    "        'foot': ['ft', 'foot'],\n",
    "        'inch': ['in', 'inch', '\"'],\n",
    "        'metre': ['m', 'metre', 'meter'],\n",
    "        'millimetre': ['mm', 'millimetre', 'millimeter'],\n",
    "        'yard': ['yd', 'yard']\n",
    "    }    \n",
    "    return 0\n",
    "\n",
    "def handle_depth(image, category_id):\n",
    "    keywords = {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'}\n",
    "    return 0\n",
    "\n",
    "def handle_height(image, category_id):\n",
    "    keywords = {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'}\n",
    "    return 0\n",
    "\n",
    "def handle_item_weight(image_location, category_id):\n",
    "    unit_map = {\n",
    "        'gram': ['g', 'gram', 'gm'],\n",
    "        'kilogram': ['kg', 'kilogram'],\n",
    "        'microgram': ['mcg', 'microgram', 'μg'],\n",
    "        'milligram': ['mg', 'milligram'],\n",
    "        'ounce': ['oz', 'ounce'],\n",
    "        'pound': ['ibs', 'Ibs', 'lb', 'lbs', 'pound'],\n",
    "        'ton': ['ton', 't']\n",
    "    }\n",
    "\n",
    "    unit_map_reversed = {unit: full_form for full_form, units in unit_map.items() for unit in units}\n",
    "\n",
    "    unit_patterns = '|'.join([re.escape(u) for u in unit_map_reversed.keys()])\n",
    "\n",
    "    img = Image.open(image_location).convert('L')\n",
    "    result = reader.readtext(np.asarray(img))\n",
    "    \n",
    "    text = \"\"\n",
    "    for (bbox, t, prob) in result:\n",
    "        text = text + \" \" + t\n",
    "    \n",
    "    #print(text)  \n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    pattern = rf'(\\d+\\.?\\d*)\\s*({unit_patterns})'\n",
    "    \n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    result = []\n",
    "    for match in matches:\n",
    "        value, unit = match[0], match[1].lower()\n",
    "        \n",
    "        if float(value) == 0:\n",
    "            continue\n",
    "        \n",
    "        if unit in unit_map_reversed:\n",
    "            full_form = unit_map_reversed[unit]\n",
    "            result.append(f\"{value} {full_form}\")\n",
    "\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def handle_maximum_weight_recommendation(image_location, category_id):\n",
    "    unit_map = {\n",
    "        'gram': ['g', 'gram', 'gm'],\n",
    "        'kilogram': ['kg', 'kilogram', 'kcs'],\n",
    "        'microgram': ['mcg', 'microgram', 'μg'],\n",
    "        'milligram': ['mg', 'milligram'],\n",
    "        'ounce': ['oz', 'ounce'],\n",
    "        'pound': ['ibs', 'Ibs', 'lb', 'lbs', 'pound'],\n",
    "        'ton': ['ton', 't']\n",
    "    }\n",
    "\n",
    "    unit_map_reversed = {unit: full_form for full_form, units in unit_map.items() for unit in units}\n",
    "\n",
    "    unit_patterns = '|'.join([re.escape(u) for u in unit_map_reversed.keys()])\n",
    "\n",
    "    def extract_text(image):\n",
    "        img = Image.open(image).convert('L')\n",
    "        result = reader.readtext(np.asarray(img))\n",
    "\n",
    "        text = \" \".join([t for (_, t, _) in result])\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def find_max_weight(text):\n",
    "        pattern = rf'(\\d+\\.?\\d*)\\s*({unit_patterns})'\n",
    "        \n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "        max_value = None\n",
    "        max_unit = None\n",
    "\n",
    "        for match in matches:\n",
    "            value, unit = match[0], match[1].lower()\n",
    "\n",
    "            value = float(value)\n",
    "\n",
    "            \n",
    "            if unit in unit_map_reversed:\n",
    "                full_form = unit_map_reversed[unit]\n",
    "\n",
    "                \n",
    "                if max_value is None or value > max_value:\n",
    "                    max_value = value\n",
    "                    max_unit = full_form\n",
    "\n",
    "        return max_value, max_unit\n",
    "\n",
    "    \n",
    "    text = extract_text(image_location)\n",
    "    max_value, max_unit = find_max_weight(text)\n",
    "\n",
    "    \n",
    "    if max_value is None:\n",
    "        img = Image.open(image_location)\n",
    "        \n",
    "        img_resized = img.resize((img.width * 2, img.height * 2), Image.Resampling.LANCZOS)\n",
    "\n",
    "        \n",
    "        result = reader.readtext(np.asarray(img_resized))\n",
    "\n",
    "        \n",
    "        text = \" \".join([t for (_, t, _) in result])\n",
    "\n",
    "        \n",
    "        max_value, max_unit = find_max_weight(text)\n",
    "\n",
    "    \n",
    "    if max_value is not None:\n",
    "        return f\"{max_value} {max_unit}\"\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def handle_voltage(image_location, category_id):\n",
    "    \n",
    "    unit_map = {\n",
    "        'kilovolt': ['kV', 'kilovolt', 'kv'],\n",
    "        'millivolt': ['mV', 'millivolt', 'mv'],\n",
    "        'volt': ['V', 'volt', 'v']\n",
    "    }\n",
    "\n",
    "    \n",
    "    unit_map_reversed = {unit: full_form for full_form, units in unit_map.items() for unit in units}\n",
    "\n",
    "    \n",
    "    unit_patterns = '|'.join([re.escape(u) for u in unit_map_reversed.keys()])\n",
    "\n",
    "    \n",
    "    img = Image.open(image_location).convert('L')\n",
    "    result = reader.readtext(np.asarray(img))\n",
    "    \n",
    "    text = \"\"\n",
    "    for (bbox, t, prob) in result:\n",
    "        text = text + \" \" + t\n",
    "        \n",
    "\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    \n",
    "    pattern = rf'(\\d+\\.?\\d*)\\s*({unit_patterns})'\n",
    "\n",
    "    \n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    \n",
    "    result = []\n",
    "    for match in matches:\n",
    "        value, unit = match[0], match[1].lower()\n",
    "\n",
    "        \n",
    "        if unit in unit_map_reversed:\n",
    "            full_form = unit_map_reversed[unit]\n",
    "            result.append(f\"{value} {full_form}\")\n",
    "\n",
    "    \n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def handle_wattage(image_location, category_id):\n",
    "    \n",
    "    unit_map = {\n",
    "        'kilowatt': ['kW', 'kilowatt', 'kw'],\n",
    "        'watt': ['W', 'watt', 'w']\n",
    "    }\n",
    "\n",
    "    \n",
    "    unit_map_reversed = {unit: full_form for full_form, units in unit_map.items() for unit in units}\n",
    "\n",
    "    \n",
    "    unit_patterns = '|'.join([re.escape(u) for u in unit_map_reversed.keys()])\n",
    "\n",
    "    \n",
    "    img = Image.open(image_location).convert('L')\n",
    "    result = reader.readtext(np.asarray(img))\n",
    "    text = \"\"\n",
    "    for (bbox, t, prob) in result:\n",
    "        text = text + \" \" + t\n",
    "        \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    \n",
    "    pattern = rf'(\\d+\\.?\\d*)\\s*({unit_patterns})'\n",
    "\n",
    "    \n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    \n",
    "    result = []\n",
    "    for match in matches:\n",
    "        value, unit = match[0], match[1].lower()\n",
    "\n",
    "        \n",
    "        if unit in unit_map_reversed:\n",
    "            full_form = unit_map_reversed[unit]\n",
    "            result.append(f\"{value} {full_form}\")\n",
    "\n",
    "    \n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def handle_item_volume(image_location, category_id):\n",
    "    \n",
    "    unit_map = {\n",
    "        'centilitre': ['cl', 'centilitre'],\n",
    "        'cubic foot': ['cubic foot', 'ft³'],\n",
    "        'cubic inch': ['cubic inch', 'in³'],\n",
    "        'cup': ['cup'],\n",
    "        'decilitre': ['dl', 'decilitre'],\n",
    "        'fluid ounce': ['Fl','Fl 0z','fl oz','fl. oz','fi. oz', 'fluid ounce', 'oz'],\n",
    "        'gallon': ['gallon', 'gal'],\n",
    "        'imperial gallon': ['imperial gallon', 'imp gal'],\n",
    "        'litre': ['litre', 'liter', 'l'],\n",
    "        'microlitre': ['microlitre', 'µl'],\n",
    "        'millilitre': ['millilitre', 'ml'],\n",
    "        'pint': ['pint', 'pt'],\n",
    "        'quart': ['quart', 'qt']\n",
    "    }\n",
    "\n",
    "    \n",
    "    unit_patterns = '|'.join([f\"({'|'.join(units)})\" for units in unit_map.values()])\n",
    "\n",
    "    def extract_text(image):\n",
    "        \n",
    "        img = Image.open(image).convert('L')\n",
    "        result = reader.readtext(np.asarray(img))\n",
    "\n",
    "        \n",
    "        text = \" \".join([t for (_, t, _) in result])\n",
    "        return text\n",
    "\n",
    "    def find_volume(text):\n",
    "        \n",
    "        pattern = rf'(\\d+\\.?\\d*)\\s*({unit_patterns})'\n",
    "\n",
    "        \n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "\n",
    "        \n",
    "        result = []\n",
    "        for match in matches:\n",
    "            value, unit = match[0], match[1].lower()\n",
    "\n",
    "            \n",
    "            for full_form, short_forms in unit_map.items():\n",
    "                if unit in [u.lower() for u in short_forms]:\n",
    "                    unit = full_form\n",
    "                    break\n",
    "\n",
    "            result.append(f\"{value} {unit}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    \n",
    "    text = extract_text(image_location)\n",
    "    result = find_volume(text)\n",
    "\n",
    "    \n",
    "    if not result:\n",
    "        \n",
    "        filtered_text = text  \n",
    "        result = find_volume(filtered_text)\n",
    "\n",
    "    \n",
    "    if not result:\n",
    "        img = Image.open(image_location)\n",
    "        \n",
    "        img_resized = img.resize((img.width * 2, img.height * 2), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        resized_image_location = \"resized_image.jpg\"\n",
    "        img_resized.save(resized_image_location)\n",
    "        \n",
    "        text = extract_text(resized_image_location)\n",
    "        result = find_volume(text)\n",
    "\n",
    "    \n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def predictor(image_link, category_id, entity_name):\n",
    "    '''\n",
    "    Call your model/approach here\n",
    "    '''\n",
    "    \n",
    "    if entity_name == 'width':\n",
    "        return handle_width(image_link, category_id)\n",
    "    elif entity_name == 'depth':\n",
    "        return handle_depth(image_link, category_id)\n",
    "    elif entity_name == 'height':\n",
    "        return handle_height(image_link, category_id)\n",
    "    elif entity_name == 'item_weight':\n",
    "        return handle_item_weight(image_link, category_id)\n",
    "    elif entity_name == 'maximum_weight_recommendation':\n",
    "        return handle_maximum_weight_recommendation(image_link, category_id)\n",
    "    elif entity_name == 'voltage':\n",
    "        return handle_voltage(image_link, category_id)\n",
    "    elif entity_name == 'wattage':\n",
    "        return handle_wattage(image_link, category_id)\n",
    "    elif entity_name == 'item_volume':\n",
    "        return handle_item_volume(image_link, category_id)\n",
    "    else:\n",
    "        return \"Invalid entity name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681b2e8-7988-4978-a55c-3123b141359a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preliminary Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737cdb1d-c0f2-46e9-a04d-936b5aacdd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'16 fluid ounce'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_item_volume(\"Image1.jpg\",3333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e508f29f-4416-49b1-8f34-fcab09739354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4 fluid ounce'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_item_volume(\"VolumeTest.jpg\",3333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c5e758-d61d-41c3-87a8-44f9ce784be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(handle_width(\"Width.jpg\",123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a3b8e7-a356-467a-848e-c4a0087adcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 watt\n"
     ]
    }
   ],
   "source": [
    "print(handle_wattage(\"Wattage.jpg\",222))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "596e2c73-6068-4701-a613-dbd1ed43efe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 watt\n"
     ]
    }
   ],
   "source": [
    "print(handle_wattage(\"Wattage2.jpg\",222))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41aca015-0f46-4bcc-b38a-fbd6e34d610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 volt\n"
     ]
    }
   ],
   "source": [
    "print(handle_voltage(\"Wattage.jpg\",444))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db6f4742-d746-4f88-ab84-52637d1da116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 gram\n"
     ]
    }
   ],
   "source": [
    "print(handle_item_weight(\"Weight.jpg\",444))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b577efdb-8428-46a4-8405-b4de6226428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(handle_item_weight(\"Test2.jpg\",444))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7c60c78-f0d6-4113-a2c3-5a7bec2c2cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0 kilogram\n"
     ]
    }
   ],
   "source": [
    "print(handle_maximum_weight_recommendation(\"MaxWeight.jpg\",444))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d64fe3a-2ad9-42e3-88d0-270042079031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350.0 pound\n"
     ]
    }
   ],
   "source": [
    "print(handle_maximum_weight_recommendation(\"MaxWeight2.jpg\",444))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f8aa99-c41f-4d9e-85d0-7bb50d69badf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0 kilogram\n"
     ]
    }
   ],
   "source": [
    "print(handle_maximum_weight_recommendation(\"MaxWeight3.jpg\",444))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8324bc39-5045-45e2-af08-6058ac43dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 milligram\n"
     ]
    }
   ],
   "source": [
    "print(handle_maximum_weight_recommendation(\"MaxWeight4.jpg\",444))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d803e8c-1021-46f6-87bf-9a4ab8c68aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(handle_maximum_weight_recommendation(\"WeightTest.jpg\",444))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447e96f-5c39-4503-bb1e-74d5dd9bf73e",
   "metadata": {},
   "source": [
    "# The runner code to download the image, and run it through the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a9ed5d-6917-44be-b93a-f9f1048d1c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41-NCxNuBx...</td>\n",
       "      <td>658003</td>\n",
       "      <td>width</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41-NCxNuBx...</td>\n",
       "      <td>658003</td>\n",
       "      <td>depth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://m.media-amazon.com/images/I/417NJrPEk+...</td>\n",
       "      <td>939426</td>\n",
       "      <td>maximum_weight_recommendation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://m.media-amazon.com/images/I/417SThj+Sr...</td>\n",
       "      <td>276700</td>\n",
       "      <td>voltage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://m.media-amazon.com/images/I/417SThj+Sr...</td>\n",
       "      <td>276700</td>\n",
       "      <td>wattage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81IYdOV0mV...</td>\n",
       "      <td>721522</td>\n",
       "      <td>maximum_weight_recommendation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81PG3ea0MO...</td>\n",
       "      <td>240413</td>\n",
       "      <td>voltage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81aZ2ozp1G...</td>\n",
       "      <td>805279</td>\n",
       "      <td>maximum_weight_recommendation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81qUmRUUTT...</td>\n",
       "      <td>603688</td>\n",
       "      <td>maximum_weight_recommendation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>https://m.media-amazon.com/images/I/81qUmRUUTT...</td>\n",
       "      <td>603688</td>\n",
       "      <td>item_weight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                         image_link  group_id  \\\n",
       "0       0  https://m.media-amazon.com/images/I/41-NCxNuBx...    658003   \n",
       "1       1  https://m.media-amazon.com/images/I/41-NCxNuBx...    658003   \n",
       "2       2  https://m.media-amazon.com/images/I/417NJrPEk+...    939426   \n",
       "3       3  https://m.media-amazon.com/images/I/417SThj+Sr...    276700   \n",
       "4       4  https://m.media-amazon.com/images/I/417SThj+Sr...    276700   \n",
       "..    ...                                                ...       ...   \n",
       "83     83  https://m.media-amazon.com/images/I/81IYdOV0mV...    721522   \n",
       "84     84  https://m.media-amazon.com/images/I/81PG3ea0MO...    240413   \n",
       "85     85  https://m.media-amazon.com/images/I/81aZ2ozp1G...    805279   \n",
       "86     86  https://m.media-amazon.com/images/I/81qUmRUUTT...    603688   \n",
       "87     87  https://m.media-amazon.com/images/I/81qUmRUUTT...    603688   \n",
       "\n",
       "                      entity_name  \n",
       "0                           width  \n",
       "1                           depth  \n",
       "2   maximum_weight_recommendation  \n",
       "3                         voltage  \n",
       "4                         wattage  \n",
       "..                            ...  \n",
       "83  maximum_weight_recommendation  \n",
       "84                        voltage  \n",
       "85  maximum_weight_recommendation  \n",
       "86  maximum_weight_recommendation  \n",
       "87                    item_weight  \n",
       "\n",
       "[88 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   0%|                                                                        | 0/88 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0, prediction: 0\n",
      "index: 1, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   3%|██▏                                                             | 3/88 [00:00<00:22,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 2, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   5%|██▉                                                             | 4/88 [00:01<00:29,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 3, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   6%|███▋                                                            | 5/88 [00:01<00:34,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 4, prediction: 2100 watt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:   8%|█████                                                           | 7/88 [00:02<00:24,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 5, prediction: 0\n",
      "index: 6, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  11%|███████▏                                                       | 10/88 [00:02<00:13,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 7, prediction: 55 watt\n",
      "index: 8, prediction: 0\n",
      "index: 9, prediction: 0\n",
      "index: 10, prediction: 0\n",
      "index: 11, prediction: 0\n",
      "index: 12, prediction: 0\n",
      "index: 13, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  17%|██████████▋                                                    | 15/88 [00:03<00:09,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 14, prediction: 6.75 pound\n",
      "index: 15, prediction: 0\n",
      "index: 16, prediction: 0\n",
      "index: 17, prediction: 0\n",
      "index: 18, prediction: 0\n",
      "index: 19, prediction: 0\n",
      "index: 20, prediction: 0\n",
      "index: 21, prediction: 0\n",
      "index: 22, prediction: 0\n",
      "index: 23, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  28%|█████████████████▉                                             | 25/88 [00:03<00:03, 16.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 24, prediction: 12 volt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  31%|███████████████████▎                                           | 27/88 [00:03<00:04, 13.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 25, prediction: 0\n",
      "index: 26, prediction: 0\n",
      "index: 27, prediction: 0\n",
      "index: 28, prediction: 0\n",
      "index: 29, prediction: 0\n",
      "index: 30, prediction: 0\n",
      "index: 31, prediction: 0\n",
      "index: 32, prediction: 0\n",
      "index: 33, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  42%|██████████████████████████▍                                    | 37/88 [00:04<00:03, 15.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 34, prediction: 75.0 pound\n",
      "index: 35, prediction: 50.0 ton\n",
      "index: 36, prediction: 0\n",
      "index: 37, prediction: 0\n",
      "index: 38, prediction: 0\n",
      "index: 39, prediction: 0\n",
      "index: 40, prediction: 0\n",
      "index: 41, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  49%|██████████████████████████████▊                                | 43/88 [00:04<00:02, 16.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 42, prediction: 43.0 pound\n",
      "index: 43, prediction: 684 volt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  51%|████████████████████████████████▏                              | 45/88 [00:05<00:04,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 44, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  53%|█████████████████████████████████▋                             | 47/88 [00:05<00:04,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 45, prediction: 0\n",
      "index: 46, prediction: 0\n",
      "index: 47, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  56%|███████████████████████████████████                            | 49/88 [00:06<00:05,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 48, prediction: 71 ton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  57%|███████████████████████████████████▊                           | 50/88 [00:06<00:07,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 49, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  58%|████████████████████████████████████▌                          | 51/88 [00:07<00:08,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 50, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  59%|█████████████████████████████████████▏                         | 52/88 [00:07<00:09,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 51, prediction: 0\n",
      "index: 52, prediction: 0\n",
      "index: 53, prediction: 0\n",
      "index: 54, prediction: 0\n",
      "index: 55, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  65%|████████████████████████████████████████▊                      | 57/88 [00:08<00:06,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 56, prediction: 120.0 volt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  66%|█████████████████████████████████████████▌                     | 58/88 [00:08<00:08,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 57, prediction: 88 watt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  67%|██████████████████████████████████████████▏                    | 59/88 [00:09<00:09,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 58, prediction: 0\n",
      "index: 59, prediction: 0\n",
      "index: 60, prediction: 0\n",
      "index: 61, prediction: 0\n",
      "index: 62, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  73%|█████████████████████████████████████████████▊                 | 64/88 [00:10<00:06,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 63, prediction: 5 gram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  74%|██████████████████████████████████████████████▌                | 65/88 [00:11<00:08,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 64, prediction: 5.0 gram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  75%|███████████████████████████████████████████████▎               | 66/88 [00:11<00:08,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 65, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  76%|███████████████████████████████████████████████▉               | 67/88 [00:12<00:09,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 66, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  77%|████████████████████████████████████████████████▋              | 68/88 [00:13<00:10,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 67, prediction: 500 watt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  78%|█████████████████████████████████████████████████▍             | 69/88 [00:14<00:10,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 68, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  80%|██████████████████████████████████████████████████             | 70/88 [00:14<00:10,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 69, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  81%|██████████████████████████████████████████████████▊            | 71/88 [00:18<00:24,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 70, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  82%|███████████████████████████████████████████████████▌           | 72/88 [00:19<00:21,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 71, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  83%|████████████████████████████████████████████████████▎          | 73/88 [00:20<00:18,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 72, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  84%|████████████████████████████████████████████████████▉          | 74/88 [00:21<00:16,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 73, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  85%|█████████████████████████████████████████████████████▋         | 75/88 [00:22<00:14,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 74, prediction: 110 watt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  86%|██████████████████████████████████████████████████████▍        | 76/88 [00:23<00:12,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 75, prediction: 10.0 ton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  88%|███████████████████████████████████████████████████████▏       | 77/88 [00:24<00:10,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 76, prediction: 10 ton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  89%|███████████████████████████████████████████████████████▊       | 78/88 [00:26<00:11,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 77, prediction: 4.7 gram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  90%|████████████████████████████████████████████████████████▌      | 79/88 [00:27<00:09,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 78, prediction: 60 watt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  91%|█████████████████████████████████████████████████████████▎     | 80/88 [00:28<00:08,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 79, prediction: 265 volt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  92%|█████████████████████████████████████████████████████████▉     | 81/88 [00:28<00:06,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 80, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  93%|██████████████████████████████████████████████████████████▋    | 82/88 [00:30<00:06,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 81, prediction: 2.1 kilogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  94%|███████████████████████████████████████████████████████████▍   | 83/88 [00:31<00:05,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 82, prediction: 660.0 pound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  95%|████████████████████████████████████████████████████████████▏  | 84/88 [00:32<00:05,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 83, prediction: 1500.0 pound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  97%|████████████████████████████████████████████████████████████▊  | 85/88 [00:34<00:04,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 84, prediction: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  98%|█████████████████████████████████████████████████████████████▌ | 86/88 [00:38<00:04,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 85, prediction: 500.0 pound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images:  99%|██████████████████████████████████████████████████████████████▎| 87/88 [00:39<00:01,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 86, prediction: 350.0 pound\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|███████████████████████████████████████████████████████████████| 88/88 [00:40<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 87, prediction: 350 pound\n",
      "Final save: Remaining 88 predictions saved to sampletestresult_part_final.csv\n",
      "All partial files concatenated into resultshalf.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm  \n",
    "from PIL import Image\n",
    "import glob  \n",
    "\n",
    "def create_placeholder_image(image_path, width=100, height=100):\n",
    "    \"\"\"\n",
    "    Create a black placeholder image if the original image fails to download.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: The path where the placeholder image will be saved.\n",
    "    - width: The width of the placeholder image (default is 100 pixels).\n",
    "    - height: The height of the placeholder image (default is 100 pixels).\n",
    "    \"\"\"\n",
    "    placeholder = Image.new('RGB', (width, height), color='black')\n",
    "    \n",
    "    placeholder.save(image_path)\n",
    "\n",
    "def download_image(image_link, save_folder, retries=3, delay=3):\n",
    "    if not isinstance(image_link, str):\n",
    "        return\n",
    "\n",
    "    filename = Path(image_link).name\n",
    "    image_save_path = os.path.join(save_folder, filename)\n",
    "\n",
    "    if os.path.exists(image_save_path):\n",
    "        return image_save_path\n",
    "\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(image_link, image_save_path)\n",
    "            return image_save_path\n",
    "        except:\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    create_placeholder_image(image_save_path)  \n",
    "    return image_save_path\n",
    "\n",
    "results_df = pd.DataFrame(columns=['index', 'prediction'])\n",
    "\n",
    "def download_and_predict(image_link, save_folder, category_id, entity_name, retries=3, delay=3):\n",
    "    image_save_path = download_image(image_link, save_folder, retries, delay)\n",
    "    prediction = predictor(image_save_path, category_id, entity_name)\n",
    "    return prediction\n",
    "\n",
    "def process_images_from_csv(csv_path, download_folder, output_csv_path, save_interval=5000):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    display(df)\n",
    "    print(df.shape)\n",
    "\n",
    "    image_links = df['image_link'].tolist()\n",
    "    categories = df['group_id'].tolist()\n",
    "    entities = df['entity_name'].tolist()\n",
    "    indices = df['index'].tolist()  \n",
    "\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, (image_link, category_id, entity_name, index) in enumerate(tqdm(zip(image_links, categories, entities, indices), total=len(image_links), desc=\"Processing Images\")):\n",
    "        \n",
    "        prediction = download_and_predict(image_link, download_folder, category_id, entity_name)\n",
    "        print(f'index: {index}, prediction: {prediction}')\n",
    "        results.append({'index': index, 'prediction': prediction})\n",
    "\n",
    "        if (i + 1) % save_interval == 0:\n",
    "            results_df = pd.DataFrame(results)\n",
    "            \n",
    "            partial_output_csv = output_csv_path.replace(\".csv\", f\"_part_{(i + 1) // save_interval}.csv\")\n",
    "            results_df.to_csv(partial_output_csv, index=False)\n",
    "            print(f\"Saved progress after {i + 1} predictions to {partial_output_csv}\")\n",
    "            \n",
    "            results.clear()\n",
    "\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        partial_output_csv = output_csv_path.replace(\".csv\", f\"_part_final.csv\")\n",
    "        results_df.to_csv(partial_output_csv, index=False)\n",
    "        print(f\"Final save: Remaining {len(results)} predictions saved to {partial_output_csv}\")\n",
    "\n",
    "        concatenate_results(output_csv_path)\n",
    "\n",
    "def concatenate_results(final_output_csv):\n",
    "    \"\"\"\n",
    "    Concatenate all partial CSV files into one final file.\n",
    "    \n",
    "    Parameters:\n",
    "    - final_output_csv: The path where the final concatenated CSV will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    partial_csvs = glob.glob(final_output_csv.replace(\".csv\", \"_part_*.csv\"))\n",
    "    \n",
    "    partial_csvs.sort()\n",
    "\n",
    "    combined_df = pd.concat([pd.read_csv(csv_file) for csv_file in partial_csvs])\n",
    "\n",
    "    combined_df.to_csv('resultshalf.csv', index=False)\n",
    "    print(f\"All partial files concatenated into resultshalf.csv\")\n",
    "\n",
    "process_images_from_csv('sample_test.csv', 'download_folder', 'sampletestresult.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1563e-2601-48b7-ae67-81d01ed82302",
   "metadata": {},
   "source": [
    "# Use this to concatenate the partial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84c26c75-b43f-436f-9416-59bb03c029a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\Akshay Kumar\\Pyth\\Amazon ML Challenge\\halfresults_part_1.csv\n",
      "Processing C:\\Users\\Akshay Kumar\\Pyth\\Amazon ML Challenge\\halfresults_part_2.csv\n",
      "Processing C:\\Users\\Akshay Kumar\\Pyth\\Amazon ML Challenge\\halfresults_part_3.csv\n",
      "Processing C:\\Users\\Akshay Kumar\\Pyth\\Amazon ML Challenge\\halfresults_part_4.csv\n",
      "Processing C:\\Users\\Akshay Kumar\\Pyth\\Amazon ML Challenge\\halfresults_part_5.csv\n",
      "Processing C:\\Users\\Akshay Kumar\\Pyth\\Amazon ML Challenge\\halfresults_part_6.csv\n",
      "Processing C:\\Users\\Akshay Kumar\\Pyth\\Amazon ML Challenge\\halfresults_part_7.csv\n",
      "Concatenated file saved as C:\\Users\\Akshay Kumar\\Pyth\\Amazon ML Challenge\\combined_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "folder_path = os.getcwd()\n",
    "\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"halfresults_part_*.csv\"))\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file_path in sorted(csv_files):  \n",
    "    print(f\"Processing {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_list.append(df)\n",
    "\n",
    "if df_list:\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Save the concatenated dataframe to a new CSV file\n",
    "    output_file = os.path.join(folder_path, \"combined_results.csv\")\n",
    "    concatenated_df.to_csv(output_file, index=False)\n",
    "    print(f\"Concatenated file saved as {output_file}\")\n",
    "else:\n",
    "    print(\"No matching CSV files found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3868cec2-f7cc-42b6-83a7-f993176d9603",
   "metadata": {},
   "source": [
    "# Use this to create the final submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a73963b0-7d4d-4181-bc43-f97b83c235cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://m.media-amazon.com/images/I/110EibNycl...</td>\n",
       "      <td>156839</td>\n",
       "      <td>height</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11TU2clswz...</td>\n",
       "      <td>792578</td>\n",
       "      <td>width</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11TU2clswz...</td>\n",
       "      <td>792578</td>\n",
       "      <td>height</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11TU2clswz...</td>\n",
       "      <td>792578</td>\n",
       "      <td>depth</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://m.media-amazon.com/images/I/11gHj8dhhr...</td>\n",
       "      <td>792578</td>\n",
       "      <td>depth</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131182</th>\n",
       "      <td>131283</td>\n",
       "      <td>https://m.media-amazon.com/images/I/A1rVsIzEtk...</td>\n",
       "      <td>721522</td>\n",
       "      <td>maximum_weight_recommendation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131183</th>\n",
       "      <td>131284</td>\n",
       "      <td>https://m.media-amazon.com/images/I/A1rdvZ5zDd...</td>\n",
       "      <td>603688</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131184</th>\n",
       "      <td>131285</td>\n",
       "      <td>https://m.media-amazon.com/images/I/A1rdvZ5zDd...</td>\n",
       "      <td>603688</td>\n",
       "      <td>maximum_weight_recommendation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131185</th>\n",
       "      <td>131286</td>\n",
       "      <td>https://m.media-amazon.com/images/I/A1tnTUPyr7...</td>\n",
       "      <td>853009</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131186</th>\n",
       "      <td>131287</td>\n",
       "      <td>https://m.media-amazon.com/images/I/A1tnTUPyr7...</td>\n",
       "      <td>853009</td>\n",
       "      <td>maximum_weight_recommendation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131187 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                         image_link  group_id  \\\n",
       "0            0  https://m.media-amazon.com/images/I/110EibNycl...    156839   \n",
       "1            1  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
       "2            2  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
       "3            3  https://m.media-amazon.com/images/I/11TU2clswz...    792578   \n",
       "4            4  https://m.media-amazon.com/images/I/11gHj8dhhr...    792578   \n",
       "...        ...                                                ...       ...   \n",
       "131182  131283  https://m.media-amazon.com/images/I/A1rVsIzEtk...    721522   \n",
       "131183  131284  https://m.media-amazon.com/images/I/A1rdvZ5zDd...    603688   \n",
       "131184  131285  https://m.media-amazon.com/images/I/A1rdvZ5zDd...    603688   \n",
       "131185  131286  https://m.media-amazon.com/images/I/A1tnTUPyr7...    853009   \n",
       "131186  131287  https://m.media-amazon.com/images/I/A1tnTUPyr7...    853009   \n",
       "\n",
       "                          entity_name prediction  \n",
       "0                              height        NaN  \n",
       "1                               width        NaN  \n",
       "2                              height        NaN  \n",
       "3                               depth        NaN  \n",
       "4                               depth        NaN  \n",
       "...                               ...        ...  \n",
       "131182  maximum_weight_recommendation        NaN  \n",
       "131183                    item_weight        NaN  \n",
       "131184  maximum_weight_recommendation        NaN  \n",
       "131185                    item_weight        NaN  \n",
       "131186  maximum_weight_recommendation        NaN  \n",
       "\n",
       "[131187 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created as submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "results_df = pd.read_csv(\"combined_results.csv\")\n",
    "\n",
    "merged_df = pd.merge(test_df, results_df, on='index', how='left')\n",
    "merged_df.replace(\"0\",\"\",inplace=True)\n",
    "display(merged_df)\n",
    "submission_df = merged_df[['index', 'prediction']]\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file created as submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b6664-21cb-4311-bef3-4dd42378692c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
